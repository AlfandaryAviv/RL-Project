authorName: Shoval Frydman
experimentName: fine_tuning_DDPG
trialConcurrency: 4   # max GPUs to use simultaneously.
maxExecDuration: 160h
maxTrialNum: 300
#choice: local, remote, pai
trainingServicePlatform: local
searchSpacePath: ddpg_params.json
#choice: true, false
useAnnotation: false
tuner:
  #choice: TPE, Random, Anneal, Evolution, BatchTuner, MetisTuner
  #SMAC (SMAC should be installed through nnictl)
  builtinTunerName: TPE
  classArgs:
    #choice: maximize, minimize
    optimize_mode: minimize
trial:
  command: python nni_experiment.py
  codeDir: .
  gpuNum: 1
localConfig:
  maxTrialNumPerGpu: 4
  useActiveGpu: true